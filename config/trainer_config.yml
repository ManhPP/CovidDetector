trainer:
  cwd: /home/ # working directory
  logger: CovidCLF # logger name
  epochs: 100 # number of training epochs
  seed: 123 # randomness seed
  cuda: True # use nvidia gpu
  gpu: 0,1 # id of gpu
  save: True # save checkpoint
  load: False # load pretrained checkpoint
  pretrained_cpkt: trainer/improved-covidnet_small.pt

  log_interval: 10 # print statistics every log_interval
  model:
    name: COVIDNet_small # models name  [mobilenet_v2,COVIDNet_small]
    optimizer: # optimizer configuration
      type: Adam # optimizer type
      lr: 1e-2 # learning rate
      weight_decay: 0.000001 # weight decay
    scheduler: # learning rate scheduler
      type: ReduceLRonPlateau # type of scheduler
      scheduler_factor: 0.7 # learning rate change ratio
      scheduler_patience: 5 # patience for some epochs
      scheduler_min_lr: 2e-4 # minimum learning rate value
      scheduler_verbose: True # print if learning rate is changed
  dataloader:
    train:
      batch_size: 64 # batch size
      shuffle: True # shuffle samples after every epoch
      num_workers: 4 # number of thread for dataloader1
    val:
      batch_size: 64
      shuffle: False
      num_workers: 4
    test:
      batch_size: 1
      shuffle: False
      num_workers: 2
  dataset:
    input_data: data
    name: COVID # dataset name COVIDx or COVID_CT
    type: binary
    modality: RGB # type of modality
    preprocess_data: [2] #[1-origin,2-combine,3-he]
    dim: [224,224] # image dimension
    train:
      augmentation: True # do augmentation to video
      preprocessing: False
    val:
      augmentation: False
      preprocessing: False
    test:
      augmentation: False
      preprocessing: False
